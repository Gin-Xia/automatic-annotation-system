{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Large Language Models for Automated Image Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Background\n",
    "\n",
    "The rise of supervised learning in computer vision has created a huge demand for high-quality labeled datasets. But manual annotation is slow, expensive, and often inconsistent.\n",
    "\n",
    "The Data Annotation Bottleneck\n",
    "- High cost – Labeling data is expensive.\n",
    "- Low efficiency – It takes too long.\n",
    "- Errors & inconsistencies – Humans make mistakes.\n",
    "\n",
    "Most current systems, like YOLO, rely on supervised models to label images. But they have serious limitations:\n",
    "- Require constant fine-tuning for different tasks.\n",
    "- Depend on fixed input words, limiting flexibility.\n",
    "\n",
    "A Smarter Approach: Multimodal Large Language Models (LLMs)\n",
    "- Adapt easily to new categories – No need for retraining.\n",
    "- Understand context & semantics – Go beyond simple keywords.\n",
    "- Affordable & accessible – Many powerful LLMs offer API access, cutting costs.\n",
    "\n",
    "By combining vision and language models, we can make annotation faster, cheaper, and more accurate.\n",
    "\n",
    "## Overview of the solution\n",
    "\n",
    "Understanding Images with OwlViT\n",
    "\n",
    "- Combines text and image features to identify objects based on descriptions.\n",
    "\n",
    "Smart Segmentation with SAM\n",
    "\n",
    "- Uses text prompts to pinpoint and segment objects in images.\n",
    "\n",
    "Seamless Pipeline: OwlViT + SAM\n",
    "- OwlViT links text descriptions to objects in the image.\n",
    "- SAM precisely segments the identified objects based on OwlViT’s insights or user prompts.\n",
    "- Generates clean annotations—bounding boxes, masks, and labels—ready for AI model training.\n",
    "\n",
    "## Business Value\n",
    "Saves Time & Money\n",
    "\n",
    "- Cuts down on labeling costs and effort, making AI development more affordable and efficient.\n",
    "\n",
    "Makes AI More Accessible\n",
    "\n",
    "- Helps small businesses and research teams get high-quality labeled data without huge budgets.\n",
    "\n",
    "Drives Innovation\n",
    "\n",
    "- Speeds up AI adoption and inspires new breakthroughs in technology.\n",
    "\n",
    "Boosts Model Performance\n",
    "\n",
    "- Creates more balanced datasets, reducing bias and improving real-world accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to use the Microsoft COCO (Common Objects in Context) dataset [Lin et al., 2014] to support our analysis and modeling tasks. OwlViT and SAM are pre-trained. We use the data to fine-tuning our pipline. \n",
    "\n",
    "- Feature: Images, texts\n",
    "\n",
    "- Annotations: The image and text data will be processed separately through an Image Encoder and a Text Encoder, mapping them into a shared high-dimensional embedding space.\n",
    "\n",
    "- Labels: Segmentation masks\n",
    "\n",
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import coco dataset\n",
    "coco_annotation_path = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "coco = COCO(coco_annotation_path)\n",
    "\n",
    "def convert_bbox_format(bbox):\n",
    "    \"\"\"Convert COCO bbox [x_min, y_min, width, height] format to [x_center, y_center, width, height] format.\"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_center = x_min + width / 2\n",
    "    y_center = y_min + height / 2\n",
    "    return [x_center, y_center, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Number and names of Categories\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "num_categories = len(categories)\n",
    "category_names = [cat['name'] for cat in categories]\n",
    "print(f\"Number of Categories: {num_categories}\")\n",
    "print(f\"Names of Categories: {category_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Number of images, annotations and average number of annotations per images\n",
    "num_images = len(coco.getImgIds())\n",
    "print(f\"Number of Images: {num_images}\")\n",
    "\n",
    "num_annotations = len(coco.getAnnIds())\n",
    "print(f\"Number of Annotations: {num_annotations}\")\n",
    "\n",
    "image_ids = coco.getImgIds()\n",
    "num_objects_per_image = [len(coco.getAnnIds(imgIds=[img_id])) for img_id in image_ids]\n",
    "mean_objects_per_image = np.mean(num_objects_per_image)\n",
    "print(f\"Average Number of Annotations per Images: {mean_objects_per_image:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# objects per categories\n",
    "category_counts = collections.Counter()\n",
    "for ann in coco.loadAnns(coco.getAnnIds()):\n",
    "    category_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "for cat_id, count in category_counts.items():\n",
    "    cat_name = coco.loadCats([cat_id])[0][\"name\"]\n",
    "    print(f\"{cat_name}: {count} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The distribution of object areas\n",
    "area_list = [ann['area'] for ann in coco.loadAnns(coco.getAnnIds())]\n",
    "plt.hist(area_list, bins=50, log=True)\n",
    "plt.xlabel('Object Area')\n",
    "plt.ylabel('Number')\n",
    "plt.title('Histogram of Object Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier/abnormal sample dectection\n",
    "- Identifies and removes images that do not have any annotations.\n",
    "- Removes duplicate annotations using IoU (Intersection over Union).\n",
    "- Removes objects with area smaller than min_area\n",
    "\n",
    "## Preprocessing\n",
    "- Converts them to RGB format.\n",
    "- Normalizes the pixel values.\n",
    "- Converts COCO format [x, y, w, h] to [x1, y1, x2, y2]\n",
    "- Applies data augmentation (if enabled).\n",
    "\n",
    "## Data augmentation\n",
    "- Horizontal flipping\n",
    "- 90-degree rotation\n",
    "- Color jittering\n",
    "- Cutout augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class COCOPipeline:\n",
    "    def __init__(self, annotation_path, iou_threshold=0.9, min_area=100, augment=True):\n",
    "        \"\"\"\n",
    "        COCO Data Cleaning and Augmentation Pipeline.\n",
    "        :param annotation_path: Path to COCO JSON annotation file\n",
    "        :param iou_threshold: IoU threshold to remove duplicate annotations\n",
    "        :param min_area: Minimum area to keep an object\n",
    "        :param augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.coco = COCO(annotation_path)\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.min_area = min_area\n",
    "        self.augment = augment\n",
    "        self.filtered_images = []\n",
    "        self.filtered_annotations = []\n",
    "\n",
    "    def remove_no_annotation_images(self):\n",
    "        \"\"\"Removes images without annotations\"\"\"\n",
    "        annotated_imgs = {ann[\"image_id\"] for ann in self.coco.loadAnns(self.coco.getAnnIds())}\n",
    "        all_imgs = set(self.coco.getImgIds())\n",
    "        empty_imgs = all_imgs - annotated_imgs\n",
    "\n",
    "        print(f\"Number of images without annotations: {len(empty_imgs)}\")\n",
    "        self.filtered_images = [img for img in self.coco.loadImgs(list(annotated_imgs))]\n",
    "\n",
    "    def remove_duplicate_annotations(self):\n",
    "        \"\"\"Removes duplicate annotations (IoU > iou_threshold)\"\"\"\n",
    "        image_to_annotations = {}\n",
    "        for ann in self.coco.loadAnns(self.coco.getAnnIds()):\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in image_to_annotations:\n",
    "                image_to_annotations[img_id] = []\n",
    "            image_to_annotations[img_id].append(ann)\n",
    "\n",
    "        filtered_annotations = []\n",
    "        for img_id, anns in image_to_annotations.items():\n",
    "            if len(anns) < 2:\n",
    "                filtered_annotations.extend(anns)\n",
    "                continue  \n",
    "\n",
    "            # Get bounding boxes and categories\n",
    "            bboxes = torch.tensor([self.convert_bbox_format(ann[\"bbox\"]) for ann in anns])\n",
    "            categories = [ann[\"category_id\"] for ann in anns]\n",
    "\n",
    "            # Compute IoU matrix\n",
    "            iou_matrix = box_iou(bboxes, bboxes)\n",
    "\n",
    "            # Store indices of annotations to keep\n",
    "            keep = set(range(len(anns)))\n",
    "            for i in range(len(anns)):\n",
    "                for j in range(i + 1, len(anns)):\n",
    "                    if iou_matrix[i, j] > self.iou_threshold and categories[i] == categories[j]:\n",
    "                        if j in keep:\n",
    "                            keep.remove(j)\n",
    "\n",
    "            filtered_annotations.extend([anns[i] for i in keep])\n",
    "\n",
    "        self.filtered_annotations = filtered_annotations\n",
    "        print(f\"Number of non-duplicate annotations: {len(self.filtered_annotations)}\")\n",
    "\n",
    "    def filter_small_objects(self):\n",
    "        \"\"\"Removes objects with area smaller than min_area\"\"\"\n",
    "        self.filtered_annotations = [ann for ann in self.filtered_annotations if ann['area'] >= self.min_area]\n",
    "        print(f\"Number of annotations after filtering small objects: {len(self.filtered_annotations)}\")\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocesses an image by resizing, normalizing, and applying data augmentation\"\"\"\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.augment:\n",
    "            image = self.apply_augmentation(image)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # Resize to a fixed size\n",
    "            transforms.ToTensor(),  # Convert to PyTorch Tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "        ])\n",
    "        return transform(Image.fromarray(image))\n",
    "\n",
    "    def apply_augmentation(self, image):\n",
    "        \"\"\"Applies data augmentation using Albumentations\"\"\"\n",
    "        augment = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),  # 50% probability of horizontal flip\n",
    "            A.RandomRotate90(p=0.5),  # 90-degree rotation\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),  # Color jitter\n",
    "            A.Cutout(num_holes=3, max_h_size=20, max_w_size=20, p=0.5),  # Cutout augmentation\n",
    "        ])\n",
    "        return augment(image=image)['image']\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_bbox_format(bbox):\n",
    "        \"\"\"Converts COCO format [x, y, w, h] to [x1, y1, x2, y2]\"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        return [x, y, x + w, y + h]\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Executes the full pipeline\"\"\"\n",
    "        print(\"Starting data cleaning and augmentation...\")\n",
    "        self.remove_no_annotation_images()\n",
    "        self.remove_duplicate_annotations()\n",
    "        self.filter_small_objects()\n",
    "        print(\"Data cleaning and augmentation completed!\")\n",
    "\n",
    "# Run the COCO preprocessing pipeline\n",
    "annotation_path = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "coco_pipeline = COCOPipeline(annotation_path, augment=True)\n",
    "coco_pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "## 1. CLIP (Contrastive Language-Image Pretraining)\n",
    "\n",
    "CLIP, proposed by OpenAI, is a **multi-modal learning model** that associates **images** and **text**, enabling **zero-shot classification and retrieval**.\n",
    "\n",
    "### CLIP's Internal Structure\n",
    "\n",
    "CLIP consists of **two encoders**:\n",
    "\n",
    "- **Vision Encoder**\n",
    "    - Uses **ViT (Vision Transformer)** or ResNet to extract image features\n",
    "    - Converts images into **fixed-dimensional feature vectors** (typically 512-dimensional)\n",
    "\n",
    "- **Text Encoder**\n",
    "    - Uses a **Transformer-based architecture** (similar to GPT-2)\n",
    "    - Converts input text into **feature vectors of the same dimension** (512-dimensional)\n",
    "\n",
    "### CLIP's Computation Process\n",
    "\n",
    "1. **Input Image and Text**\n",
    "    - The image is processed through the **ViT vision encoder**, extracting **image feature vectors**\n",
    "    - The text is processed through the **Transformer text encoder**, extracting **text feature vectors**\n",
    "\n",
    "2. **Similarity Computation**\n",
    "    - Computes the **cosine similarity** between image and text features to match **the most relevant text description**\n",
    "    - Through **contrastive learning**, CLIP maximizes similarity for correct matches and minimizes similarity for incorrect ones\n",
    "\n",
    "### Limitations of CLIP\n",
    "\n",
    "- **Only performs image-text matching**, without directly **outputting bounding boxes**, making it unsuitable for object detection\n",
    "- Requires **additional region proposals**, increasing computational complexity\n",
    "\n",
    "---\n",
    "\n",
    "## 2. OwlViT (Open-World Learning Vision Transformer)\n",
    "\n",
    "OwlViT, developed by Google, is a **ViT-based model for open-vocabulary object detection**, capable of **directly predicting bounding boxes**, overcoming CLIP’s limitations.\n",
    "\n",
    "### OwlViT's Internal Structure\n",
    "\n",
    "OwlViT is an improved version of **Detection Transformer (DETR) + ViT**, consisting of three main components:\n",
    "\n",
    "1. **ViT Vision Encoder**\n",
    "    - Similar to CLIP's ViT encoder, it converts input images into **visual feature vectors (tokens)**\n",
    "    - Unlike CLIP, OwlViT requires **region-based information**, so it includes **positional encoding** to support object detection\n",
    "\n",
    "2. **Cross-Modality Transformer**\n",
    "    - Works similarly to CLIP’s **contrastive learning mechanism**, but not only computes similarity—it also **outputs object detection bounding boxes**\n",
    "    - The input text acts as a **query**, interacting with visual features to **localize target regions**\n",
    "\n",
    "3. **Detection Head**\n",
    "    - Directly predicts the **bounding box coordinates and confidence scores** for each candidate region\n",
    "    - The confidence scores are used for **Non-Maximum Suppression (NMS)** to refine the results\n",
    "\n",
    "### OwlViT's Computation Process\n",
    "\n",
    "1. **Input Image and Text**\n",
    "    - The image is processed through ViT to extract features, while the text is encoded into feature vectors\n",
    "2. **Cross-Modal Interaction**\n",
    "    - Computes the matching score between text and image features to generate candidate object regions\n",
    "3. **Bounding Box Prediction**\n",
    "    - The detection head outputs **bounding box coordinates and confidence scores**\n",
    "4. **Post-Processing**\n",
    "    - **NMS filters out low-confidence boxes** and **merges high IoU boxes** to improve accuracy\n",
    "\n",
    "### Advantages of OwlViT\n",
    "\n",
    "**Directly generates bounding boxes from text queries**, without needing region proposals like CLIP\n",
    "**Supports open-vocabulary object detection**, recognizing **zero-shot categories**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. SAM (Segment Anything Model)\n",
    "\n",
    "SAM, developed by Meta, is a **high-precision object segmentation model** that generates object masks based on **prompts**.\n",
    "\n",
    "### SAM's Internal Structure\n",
    "\n",
    "SAM consists of **three main components**:\n",
    "\n",
    "1. **ViT Vision Encoder**\n",
    "    - Like CLIP and OwlViT, SAM also uses **ViT** for image feature extraction\n",
    "    - However, SAM requires **high-resolution feature maps**, so it adopts a **high-capacity ViT variant (ViT-Huge)**\n",
    "\n",
    "2. **Prompt Encoder**\n",
    "    - SAM supports **various types of prompts**, including:\n",
    "        - **Point**: The user clicks on an object, and SAM predicts the mask for that region\n",
    "        - **Box**: The user provides a bounding box, and SAM generates a precise mask\n",
    "    - In this project, we use **OwlViT-generated bounding boxes as SAM’s input**\n",
    "\n",
    "3. **Mask Decoder**\n",
    "    - Combines **ViT visual features + prompt information** to generate the **final segmentation mask**\n",
    "\n",
    "### SAM's Computation Process\n",
    "\n",
    "1. **Input Image and Bounding Box**\n",
    "    - OwlViT first generates bounding boxes, which are then used as **prompts** for SAM\n",
    "2. **Feature Extraction**\n",
    "    - SAM extracts **high-resolution image features** using ViT\n",
    "3. **Prompt Encoding**\n",
    "    - SAM processes the bounding box information and adjusts the prediction scope\n",
    "4. **Mask Generation**\n",
    "    - The Mask Decoder outputs **high-quality object segmentation masks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection – CLIP + SAM vs. OwlViT + SAM\n",
    "\n",
    "## Initial Attempt: CLIP + SAM\n",
    "\n",
    "### Why Initially Choose CLIP?\n",
    "\n",
    "- CLIP is a powerful **vision-language model** that understands the relationship between images and text, supporting **zero-shot learning**.\n",
    "- It is well-suited for **image classification** and **open-category object recognition**, allowing it to handle **unseen categories**.\n",
    "\n",
    "### Issues with CLIP + SAM\n",
    "\n",
    "- **CLIP cannot directly provide bounding boxes**, only assessing the similarity between an entire image and a text query.\n",
    "- **Requires an additional region proposal algorithm**:\n",
    "    - Since CLIP cannot directly detect objects, **Selective Search, edge detection**, or other methods must first generate **candidate regions**.\n",
    "    - Each candidate region is then matched with CLIP using **text similarity**, selecting the highest-matching region as the detection result.\n",
    "- **Multi-step process increases computational complexity**:\n",
    "    1. Generate multiple candidate regions.\n",
    "    2. Use CLIP to compute the similarity score for each region with the text query.\n",
    "    3. Select the highest-matching region as the detection result.\n",
    "    4. Use SAM for object segmentation.\n",
    "- **Accuracy limitations**:\n",
    "    - The quality of candidate region proposals determines final detection performance, making it prone to missing objects or false detections.\n",
    "    - Additional computation steps lead to slower inference speed.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Choice: OwlViT + MobileSAM\n",
    "\n",
    "### Why Switch to OwlViT?\n",
    "\n",
    "- **OwlViT is an open-vocabulary object detection model** that can **directly generate bounding boxes from text queries**, eliminating the need for additional region proposals.\n",
    "- **End-to-end object detection**: Given an image and a text query, the model directly outputs **bounding boxes**, avoiding CLIPs **multi-step processing**, improving detection speed and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow of OwlViT + MobileSAM\n",
    "\n",
    "1. **Input image + text query**.\n",
    "2. **OwlViT processes the image and text**, directly outputting **bounding boxes + confidence scores**.\n",
    "3. **Filter out low-confidence bounding boxes**, then apply **Non-Maximum Suppression (NMS)** to remove overlapping boxes.\n",
    "4. **Merge high IoU (Intersection over Union) detection boxes**, improving stability.\n",
    "5. **MobileSAM receives the final bounding boxes** and generates high-quality segmentation masks.\n",
    "6. **Final output**: Image with accurately segmented objects.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Is OwlViT + SAM Superior?\n",
    "\n",
    "| **Comparison** | **CLIP + SAM** | **OwlViT + SAM** |\n",
    "| --- | --- | --- |\n",
    "| **Zero-shot detection ability** | (Image-level only) |  (Object-level, direct detection) |\n",
    "| **Can output bounding boxes?** |  Requires extra steps |  Direct output |\n",
    "| **Requires region proposals?** |  Yes |  No |\n",
    "| **Merging high IoU boxes** |  No |  Using NMS and box merging |\n",
    "| **Detection accuracy** |  Depends on region proposal quality |  Optimized for precision |\n",
    "| **Computation speed** |  High due to extra steps |  More efficient |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "\n",
    "Our goal is to evaluate the **object detection** and **segmentation performance** of **OwlViT + MobileSAM**. The evaluation focuses on three key aspects: **detection accuracy, segmentation quality, and computational efficiency**.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Object Detection Evaluation Metrics\n",
    "\n",
    "The detection capability of **OwlViT** determines the final segmentation quality. Therefore, we evaluate the following metrics:\n",
    "\n",
    "### **1.1 IoU (Intersection over Union)**\n",
    "- Measures the **overlap ratio** between the predicted bounding box and the ground truth bounding box:\n",
    "$$\n",
    "\\text{IoU} = \\frac{\\text{Area of intersection}}{\\text{Area of union}}\n",
    "$$\n",
    "- **IoU > 0.5** is considered a correct detection (standard threshold).\n",
    "- **IoU > 0.9** represents high-quality detection.\n",
    "\n",
    "### **1.2 mAP (Mean Average Precision)(Later)**\n",
    "- Evaluates object detection performance across **different IoU thresholds**, calculating the average **AP** over multiple IoU values.\n",
    "- AP is calculated as follows:\n",
    "    - Compute the **area under the Precision-Recall curve**:\n",
    "    $$\n",
    "    \\text{AP} = \\int_{0}^{1} P(R) \\, dR\n",
    "    $$\n",
    "    - The final **mAP (Mean AP)** is obtained by averaging AP across all categories.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Segmentation Evaluation Metrics\n",
    "\n",
    "**MobileSAM** is responsible for object segmentation. The evaluation metrics include:\n",
    "\n",
    "### **2.1 mIoU (Mean Intersection over Union)**\n",
    "- Computes the average IoU between the predicted mask and the ground truth mask:\n",
    " $$\n",
    "\\text{mIoU} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{Area of intersection}}{\\text{Area of union}}\n",
    "$$\n",
    "- **mIoU > 0.8** represents high-quality segmentation.\n",
    "\n",
    "### **2.2 Dice Coefficient**\n",
    "- Measures the similarity between two regions:\n",
    "$$\n",
    "\\text{Dice} = \\frac{2 \\times |A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "- A Dice score closer to 1 indicates better segmentation performance.\n",
    "---\n",
    "\n",
    "## 3. Computational Efficiency\n",
    "\n",
    "### **3.1 FPS (Frames Per Second)**\n",
    "- Measures **the number of images processed per second**, assessing the system’s real-time performance:\n",
    "$$\n",
    "\\text{FPS} = \\frac{\\text{Number of processed images}}{\\text{Total time (seconds)}}\n",
    "$$\n",
    "- **Target values**:\n",
    "    - FPS > 2 for batch processing\n",
    "    - FPS > 10 for real-time applications\n",
    "\n",
    "### **3.2 Inference Time**\n",
    "- Measures the total inference time for **OwlViT + MobileSAM** on a **single image**:\n",
    "$$\n",
    "T_{\\text{total}} = T_{\\text{OwlViT}} + T_{\\text{MobileSAM}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Next Steps**\n",
    "\n",
    "After the mid-term report, our primary goal is to **optimize the post-processing pipeline for detection and segmentation to improve detection accuracy, segmentation quality, and inference efficiency**. The next optimization steps mainly include **improving confidence filtering, NMS, and high IoU box merging strategies**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Object Detection Optimization**\n",
    "\n",
    "### **1.1 NMS Strategy Optimization**\n",
    "\n",
    "Currently, `apply_nms` uses **Hard-NMS** (based on `torchvision.ops.nms()`), which directly removes high-IoU boxes. This can cause:\n",
    "\n",
    "- **False negatives**, where adjacent objects are incorrectly removed\n",
    "- **A drastic reduction in the number of detected boxes**, negatively affecting recall\n",
    "\n",
    "#### **Optimization Plan**\n",
    "\n",
    "- **Introduce Soft-NMS**\n",
    "    - Instead of directly removing boxes, apply **exponential decay** to confidence scores based on IoU:\n",
    "  \n",
    "      $$\n",
    "      \\text{scores} = \\text{scores} \\times e^{- (\\text{IoU}^2) / \\sigma}\n",
    "      $$\n",
    "\n",
    "    - **Low IoU boxes retain more confidence**, while **high IoU boxes decay more significantly**, preserving more detection information.\n",
    "\n",
    "- **Applicable scenarios**:\n",
    "    - **Crowded object detection** (e.g., pedestrian or vehicle detection)\n",
    "    - **Reduce false negatives and improve recall**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **2. Segmentation Optimization**\n",
    "\n",
    "### **2.1 Multi-Mask Optimization in MobileSAM**\n",
    "\n",
    "#### **Current Issues**\n",
    "- Using `multimask_output=False` may result in missing small objects.\n",
    "\n",
    "#### **Optimization Plan**\n",
    "- Enable `multimask_output=True` to generate multiple masks and select the best one.\n",
    "- **Objective**: Ensure complete segmentation of targets in complex scenes.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Mask Post-Processing**\n",
    "\n",
    "- **Add Mask Area Filtering**\n",
    "    - **Remove very small masks** (e.g., those with <500 pixels) to avoid detecting noise.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Evaluation Expansion**\n",
    "\n",
    "### **3.1 Testing on Different Datasets**\n",
    "\n",
    "- **Zero-Shot Evaluation**:\n",
    "    - The model has primarily been evaluated on COCO; testing on **unseen categories** is necessary.\n",
    "- **Cross-Dataset Testing**:\n",
    "    - Plan to evaluate on **LVIS and Object365** to ensure generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 More Detailed Performance Analysis**\n",
    "\n",
    "- **IoU Histogram Analysis**\n",
    "    - Analyze the distribution of predicted vs. ground truth IoU to refine detection accuracy.\n",
    "- **Segmentation Quality vs. Object Size Analysis**\n",
    "    - Observe `mIoU` performance across different object scales to enhance segmentation stability.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
