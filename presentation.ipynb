{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Large Language Models for Automated Image Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The rapid advancement of supervised learning in computer vision has led to an increasing demand for high-quality labeled datasets. However, obtaining such datasets remains a significant challenge due to the high costs and inefficiencies of manual annotation. \n",
    "\n",
    "The Bottleneck of Data Annotation\n",
    "- High cost\n",
    "- Low efficiency\n",
    "- Inconsistencies and errors\n",
    "\n",
    "Existing automated annotation systems predominantly rely on supervised models, such as the YOLO series, to generate bounding boxes and labels for images. However, these models face significant limitations:\n",
    "\n",
    "- Must fine-tuning or training a new model for subproblems\n",
    "- Require standard input words\n",
    "\n",
    "Currently, numerous well-trained large language models (LLMs) are available for commercial use, often providing accessible APIs without necessitating the substantial costs associated with pretraining such models. \n",
    "\n",
    "Leveraging Multimodal Large Language Models (LLMs) for Universal Annotation\n",
    "- Flexible adaptation to new categories\n",
    "- Semantic understanding and context awareness\n",
    "- low-cost deployment and accessibility\n",
    "\n",
    "## Overview of the solution\n",
    "\n",
    "- Multimodal Representation Learning with OwlViT\n",
    "Uses both text and image embeddings to match visual objects with text descriptions.\n",
    "- Object Detection and Segmentation with SAM\n",
    "perform text-prompted segmentation\n",
    "- Pipeline Integration: OwlViT + SAM\n",
    "Step 1: Use OwlViT to associate textual descriptions with image content.\n",
    "Step 2: Use SAM to segment specific objects or regions of interest based on OwlViT's semantic embeddings or user-provided prompts.\n",
    "Step 3: Generate annotations, including bounding boxes, masks, and descriptive labels, in a format compatible with training supervised models.\n",
    "\n",
    "## Business Value\n",
    "- Cost and Time Efficiency\n",
    "Reduces the expenses and effort required for data labeling, making AI development more accessible.\n",
    "- Democratization of AI\n",
    "Enables small enterprises and research institutions to access high-quality labeled data.\n",
    "- Fostering Innovation\n",
    "Accelerates AI adoption and encourages the development of new technologies.\n",
    "- Enhanced Model Performance\n",
    "Improves generalizability by mitigating biases through diverse datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plan to use the Conceptual Captions dataset and the Microsoft COCO (Common Objects in Context) dataset [Lin et al., 2014] to support our analysis and modeling tasks. OwlViT and Sam are pre-trained. We use the data to fine-tuning our pipline. \n",
    "\n",
    "- Large number of images along with their descriptive texts.\n",
    "\n",
    "- The image and text data will be processed separately through an Image Encoder and a Text Encoder, mapping them into a shared high-dimensional embedding space.\n",
    "\n",
    "## Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import coco dataset\n",
    "coco_annotation_path = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "coco = COCO(coco_annotation_path)\n",
    "\n",
    "def convert_bbox_format(bbox):\n",
    "    \"\"\"Convert COCO bbox [x_min, y_min, width, height] format to [x_center, y_center, width, height] format.\"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_center = x_min + width / 2\n",
    "    y_center = y_min + height / 2\n",
    "    return [x_center, y_center, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Number and names of Categories\n",
    "categories = coco.loadCats(coco.getCatIds())\n",
    "num_categories = len(categories)\n",
    "category_names = [cat['name'] for cat in categories]\n",
    "print(f\"Number of Categories: {num_categories}\")\n",
    "print(f\"Names of Categories: {category_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Number of images, annotations and average number of annotations per images\n",
    "num_images = len(coco.getImgIds())\n",
    "print(f\"Number of Images: {num_images}\")\n",
    "\n",
    "num_annotations = len(coco.getAnnIds())\n",
    "print(f\"Number of Annotations: {num_annotations}\")\n",
    "\n",
    "image_ids = coco.getImgIds()\n",
    "num_objects_per_image = [len(coco.getAnnIds(imgIds=[img_id])) for img_id in image_ids]\n",
    "mean_objects_per_image = np.mean(num_objects_per_image)\n",
    "print(f\"Average Number of Annotations per Images: {mean_objects_per_image:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# objects per categories\n",
    "category_counts = collections.Counter()\n",
    "for ann in coco.loadAnns(coco.getAnnIds()):\n",
    "    category_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "for cat_id, count in category_counts.items():\n",
    "    cat_name = coco.loadCats([cat_id])[0][\"name\"]\n",
    "    print(f\"{cat_name}: {count} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The distribution of object areas\n",
    "area_list = [ann['area'] for ann in coco.loadAnns(coco.getAnnIds())]\n",
    "plt.hist(area_list, bins=50, log=True)\n",
    "plt.xlabel('Object Area')\n",
    "plt.ylabel('Number')\n",
    "plt.title('Histogram of Object Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## outlier/abnormal sample dectection\n",
    "- Identifies and removes images that do not have any annotations.\n",
    "- Removes duplicate annotations using IoU (Intersection over Union).\n",
    "- Removes objects with area smaller than min_area\n",
    "\n",
    "## Preprocessing\n",
    "- Converts them to RGB format.\n",
    "- Normalizes the pixel values.\n",
    "- Converts COCO format [x, y, w, h] to [x1, y1, x2, y2]\n",
    "- Applies data augmentation (if enabled).\n",
    "\n",
    "## Data augmentation\n",
    "- Horizontal flipping\n",
    "- 90-degree rotation\n",
    "- Color jittering\n",
    "- Cutout augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class COCOPipeline:\n",
    "    def __init__(self, annotation_path, iou_threshold=0.9, min_area=100, augment=True):\n",
    "        \"\"\"\n",
    "        COCO Data Cleaning and Augmentation Pipeline.\n",
    "        :param annotation_path: Path to COCO JSON annotation file\n",
    "        :param iou_threshold: IoU threshold to remove duplicate annotations\n",
    "        :param min_area: Minimum area to keep an object\n",
    "        :param augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.coco = COCO(annotation_path)\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.min_area = min_area\n",
    "        self.augment = augment\n",
    "        self.filtered_images = []\n",
    "        self.filtered_annotations = []\n",
    "\n",
    "    def remove_no_annotation_images(self):\n",
    "        \"\"\"Removes images without annotations\"\"\"\n",
    "        annotated_imgs = {ann[\"image_id\"] for ann in self.coco.loadAnns(self.coco.getAnnIds())}\n",
    "        all_imgs = set(self.coco.getImgIds())\n",
    "        empty_imgs = all_imgs - annotated_imgs\n",
    "\n",
    "        print(f\"Number of images without annotations: {len(empty_imgs)}\")\n",
    "        self.filtered_images = [img for img in self.coco.loadImgs(list(annotated_imgs))]\n",
    "\n",
    "    def remove_duplicate_annotations(self):\n",
    "        \"\"\"Removes duplicate annotations (IoU > iou_threshold)\"\"\"\n",
    "        image_to_annotations = {}\n",
    "        for ann in self.coco.loadAnns(self.coco.getAnnIds()):\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in image_to_annotations:\n",
    "                image_to_annotations[img_id] = []\n",
    "            image_to_annotations[img_id].append(ann)\n",
    "\n",
    "        filtered_annotations = []\n",
    "        for img_id, anns in image_to_annotations.items():\n",
    "            if len(anns) < 2:\n",
    "                filtered_annotations.extend(anns)\n",
    "                continue  \n",
    "\n",
    "            # Get bounding boxes and categories\n",
    "            bboxes = torch.tensor([self.convert_bbox_format(ann[\"bbox\"]) for ann in anns])\n",
    "            categories = [ann[\"category_id\"] for ann in anns]\n",
    "\n",
    "            # Compute IoU matrix\n",
    "            iou_matrix = box_iou(bboxes, bboxes)\n",
    "\n",
    "            # Store indices of annotations to keep\n",
    "            keep = set(range(len(anns)))\n",
    "            for i in range(len(anns)):\n",
    "                for j in range(i + 1, len(anns)):\n",
    "                    if iou_matrix[i, j] > self.iou_threshold and categories[i] == categories[j]:\n",
    "                        if j in keep:\n",
    "                            keep.remove(j)\n",
    "\n",
    "            filtered_annotations.extend([anns[i] for i in keep])\n",
    "\n",
    "        self.filtered_annotations = filtered_annotations\n",
    "        print(f\"Number of non-duplicate annotations: {len(self.filtered_annotations)}\")\n",
    "\n",
    "    def filter_small_objects(self):\n",
    "        \"\"\"Removes objects with area smaller than min_area\"\"\"\n",
    "        self.filtered_annotations = [ann for ann in self.filtered_annotations if ann['area'] >= self.min_area]\n",
    "        print(f\"Number of annotations after filtering small objects: {len(self.filtered_annotations)}\")\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocesses an image by resizing, normalizing, and applying data augmentation\"\"\"\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.augment:\n",
    "            image = self.apply_augmentation(image)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # Resize to a fixed size\n",
    "            transforms.ToTensor(),  # Convert to PyTorch Tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "        ])\n",
    "        return transform(Image.fromarray(image))\n",
    "\n",
    "    def apply_augmentation(self, image):\n",
    "        \"\"\"Applies data augmentation using Albumentations\"\"\"\n",
    "        augment = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),  # 50% probability of horizontal flip\n",
    "            A.RandomRotate90(p=0.5),  # 90-degree rotation\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),  # Color jitter\n",
    "            A.Cutout(num_holes=3, max_h_size=20, max_w_size=20, p=0.5),  # Cutout augmentation\n",
    "        ])\n",
    "        return augment(image=image)['image']\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_bbox_format(bbox):\n",
    "        \"\"\"Converts COCO format [x, y, w, h] to [x1, y1, x2, y2]\"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        return [x, y, x + w, y + h]\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Executes the full pipeline\"\"\"\n",
    "        print(\"Starting data cleaning and augmentation...\")\n",
    "        self.remove_no_annotation_images()\n",
    "        self.remove_duplicate_annotations()\n",
    "        self.filter_small_objects()\n",
    "        print(\"Data cleaning and augmentation completed!\")\n",
    "\n",
    "# Run the COCO preprocessing pipeline\n",
    "annotation_path = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "coco_pipeline = COCOPipeline(annotation_path, augment=True)\n",
    "coco_pipeline.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
