{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eea15116",
   "metadata": {},
   "source": [
    "\n",
    "# Leveraging Large Language Models for Automated Image Annotation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ea9bf",
   "metadata": {},
   "source": [
    "\n",
    "## Authors and Team\n",
    "\n",
    "- **Author 1**: Xiao Shi, Machine Learning Engineer\n",
    "- **Author 2**: Haohao Xia\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf532e",
   "metadata": {},
   "source": [
    "\n",
    "## Abstract\n",
    "\n",
    "**Provide a brief summary of your project here (150-250 words)**\n",
    "\n",
    "The rapid advancement of supervised learning in computer vision has led to an increasing demand for high-quality labeled datasets. However, obtaining such datasets remains a significant challenge due to the high costs and inefficiencies of manual annotation. To address this issue, we propose an automated annotation system that leverages the multimodal capabilities of large language models (LLMs). By transforming traditional supervised learning pipelines into self-supervised learning frameworks, our system eliminates the need for extensive manual labeling, enabling end-to-end training with minimal human intervention. Additionally, it expands and diversifies datasets, improving model performance and accessibility, particularly for resource-constrained academic institutions and small enterprises. Our approach also enhances existing automated annotation methods by improving accuracy, adaptability, and robustness. This system not only reduces annotation costs and accelerates AI development but also promotes equitable access to high-quality data, fostering innovation in fields such as healthcare, autonomous systems, and public safety.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57a788",
   "metadata": {},
   "source": [
    "\n",
    "## Project Description\n",
    "\n",
    "### 4.1 [Problem Definition]\n",
    "\n",
    "#### i. What is the Research Problem that you will be addressing using an Analytical Framework?\n",
    "\n",
    "Supervised learning is a fundamental paradigm in machine learning and is widely applied in regression tasks and pattern recognition. However, training such models requires large-scale, high-quality labeled datasets, which are often difficult to obtain in real-world scenarios. In computer vision tasks, supervised learning has been extensively utilized, leading to a persistent demand for high-quality labeled data. Given that images inherently lack labels, extensive annotation efforts are necessary to facilitate subsequent learning processes. Traditionally, image annotation is performed manually, which is inefficient, costly, and prone to inaccuracies. To address these challenges, we propose the development of an automated annotation system.\n",
    "\n",
    "Existing automated annotation systems predominantly rely on supervised models, such as the YOLO series, to generate bounding boxes and labels for images. However, these models face significant limitations: if the model has not been trained on data that align with user-specific requirements, if the input data exhibit distributional shifts, or if the user's annotation needs deviate from structured language representations, the model may fail to produce accurate results. A common approach to address these limitations is fine-tuning the model. However, fine-tuning inherently involves additional training, which requires labeled data—creating a paradox where an automated annotation system itself requires annotated data to function effectively.\n",
    "\n",
    "Fortunately, with advancements in large-scale models, truly universal automated annotation systems have become a feasible solution. Currently, numerous well-trained large language models (LLMs) are available for commercial use, often providing accessible APIs without necessitating the substantial costs associated with pretraining such models. These language models can efficiently process textual information at low cost, enabling rapid tokenization of natural language inputs. This capability facilitates seamless alignment between textual tokens and visual tokens, significantly enhancing the feasibility of comprehensive image recognition. By integrating segmentation modules, the system can be extended to perform a wide range of annotation tasks.\n",
    "\n",
    "In summary, we aim to leverage the multimodal capabilities of contemporary large language models to develop an accurate, efficient, and cost-effective automated annotation system. This approach has the potential to scale up data annotation efforts, thereby facilitating the training of more supervised learning models and advancing the data-driven paradigm in machine learning research.\n",
    "\n",
    "#### ii. Motivation\n",
    "1. The Bottleneck of Data Annotation\n",
    "* High Cost: Hiring human annotators is resource-intensive, particularly for large-scale datasets.\n",
    "* Low Efficiency: Manual annotation is slow and cannot keep pace with the ever-growing demand for data.\n",
    "* Inconsistencies and Errors: Human annotators are prone to subjective biases and labeling inconsistencies, which can reduce the quality of the dataset and, consequently, the performance of supervised models.\n",
    "\n",
    "2. Limitations of Traditional Automated Annotation Methods\n",
    "Existing automated annotation approaches predominantly rely on supervised models, such as YOLO, Faster R-CNN, and other deep learning-based object detection frameworks. hese models require extensive pretraining on specific datasets. Moreover, many traditional annotation models struggle with semantic understanding of user-defined categories. \n",
    "\n",
    "3. Leveraging Multimodal Large Language Models (LLMs) for Universal Annotation\n",
    "* Flexible Adaptation to New Categories: LLMs can dynamically interpret user-specified categories and generate corresponding image labels, reducing reliance on predefined datasets.\n",
    "* Semantic Understanding and Context Awareness: Unlike purely visual models, LLMs can leverage contextual knowledge to enhance annotation accuracy, particularly for ambiguous or complex labeling tasks.\n",
    "* Low-Cost Deployment and Accessibility: Since many state-of-the-art LLMs are now available via APIs, organizations can integrate powerful annotation capabilities without the need for extensive computational resources or expensive model training.\n",
    "\n",
    "#### iii. What decisions will you be impacting?\n",
    "*Discuss the decisions your project will influence.*\n",
    "1. Enhancing the Supervised Learning Pipeline by Enabling Self-Supervised Learning\n",
    "Our approach seeks to transform traditional supervised learning pipelines in image processing tasks into a self-supervised learning framework. By eliminating the need for manual annotation, we aim to reduce the reliance on costly labeled datasets, thereby facilitating end-to-end training without additional labeling overhead. This transition from supervised to self-supervised learning represents a significant step toward more autonomous and scalable AI model development.\n",
    "\n",
    "2. Expanding and Diversifying Datasets to Improve Model Training\n",
    "One of the primary limitations of existing models is the scarcity of high-quality training data. Our system is designed to augment existing datasets, thereby increasing the volume and diversity of training samples. This enhancement is particularly beneficial for academic research institutions and small-scale enterprises with constrained budgets, as it provides access to a more comprehensive and varied dataset. By strengthening data infrastructure, our model can improve generalization performance and mitigate issues related to data sparsity.\n",
    "\n",
    "3. Addressing Limitations of Current Automated Annotation Systems\n",
    "While existing automated annotation systems have demonstrated promising results, they remain constrained by domain-specific limitations, model biases, and generalization challenges. Our approach seeks to improve upon these shortcomings by leveraging large multimodal models to enhance annotation accuracy, robustness, and adaptability. By incorporating contextual understanding and semantic reasoning, our system aims to outperform conventional annotation models and provide a more efficient, scalable, and reliable labeling solution.\n",
    "#### iv. What is the business/societal value of the decisions to be impacted?\n",
    "\n",
    "The development of an automated annotation system holds significant business and societal value by reducing the cost and time associated with data labeling, making AI development more accessible to small enterprises and research institutions. By democratizing access to high-quality labeled data, the system fosters innovation, accelerates AI adoption, and enhances model generalizability by mitigating biases through more diverse datasets. Additionally, it enables more efficient resource allocation, allowing researchers and engineers to focus on higher-value tasks, ultimately advancing AI-driven solutions in fields such as healthcare, autonomous systems, and public safety.\n",
    "\n",
    "#### v. Why is this project important to you?\n",
    "As students, we have experienced firsthand the challenges posed by data scarcity in machine learning research. However, due to cost constraints, acquiring large-scale annotated datasets remains a significant challenge. Our proposed system aims to provide a more cost-effective solution for data annotation, enabling more accessible and affordable labeled data, thereby alleviating the limitations imposed by insufficient annotated datasets and facilitating more efficient model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf5938",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 [Methods]\n",
    "\n",
    "#### i. How are you planning to approach the solution to your research problem?\n",
    "To address the challenges of supervised learning that demand high-quality labeled datasets, we propose a hybrid approach by combining Contrastive Language-Image Pretraining and Segment Anything Model. Our solution uses the strengths of these models to automate the annotation process, reduce manual effort, and improve scalability and adaptability. Below is a detailed plan for the solution:\n",
    "\n",
    "1. Multimodal Representation Learning with OwlViT\n",
    "\n",
    "- Use OwlViT to create a shared embedding space for images and their corresponding textual descriptions.\n",
    "- OwlViT directly outputs bounding boxes for objects based on text queries, making it better suited for localizing and detecting objects from captions.\n",
    "- Fine-tune OwlViT on a dataset like COCO or Conceptual Captions to enhance its ability to detect objects in domain-specific contexts.\n",
    "- This step allows the system to generate semantic labels and precise bounding boxes for objects, enabling zero-shot object detection.\n",
    "\n",
    "2. Object Detection and Segmentation with SAM\n",
    "\n",
    "Use SAM to identify and segment objects in images at a pixel level. By integrating OwlViT's textual guidance, SAM will perform text-prompted segmentation,such as identify objects based on user-defined textual queries. SAM ensures high-resolution, adaptable segmentation, improving annotation accuracy.\n",
    "\n",
    "3. Pipeline Integration: OwlViT + SAM\n",
    "The core pipeline changes as follows:\n",
    "\n",
    "Step 1: Use OwlViT for Object Detection\n",
    "- Input: Image + Text Query\n",
    "- Output: Bounding boxes + Object categories\n",
    "Step 2: Use SAM for Segmentation\n",
    "- Input: Bounding boxes from OwlViT\n",
    "- Output: Pixel-level segmentation masks\n",
    "Step 3: Generate Annotations\n",
    "- Store bounding boxes, segmentation masks, and descriptive labels in a structured format for downstream training.\n",
    "- Since OwlViT provides text-conditioned detection, it reduces the reliance on OwlViT’s embedding similarity and improves object-specific localization.\n",
    "\n",
    "\n",
    "4. Evaluation and Optimization\n",
    "Validate the pipeline on publicly available datasets (e.g., Conceptual Captions, COCO) and maybe domain-specific datasets.\n",
    "\n",
    "Metrics for evaluation include:\n",
    "Annotation accuracy (e.g., IoU for segmentation, label matching accuracy).\n",
    "Scalability (e.g., time and resources required for large-scale datasets).\n",
    "\n",
    "\n",
    "#### ii. What is the data asset that you plan on using?\n",
    "We plan to use Conceptual Captions dataset and the COCO dataset to support our analysis and modeling tasks:\n",
    "1. Conceptual Captions dataset\n",
    "\n",
    "This dataset contains a large number of images along with their descriptive texts. These descriptions are collected from the web, representing a diverse range of styles and content. The image and text data will be processed separately through an Image Encoder and a Text Encoder, mapping them into a shared high-dimensional embedding space. Our model will learn to establish a shared embedding space where images and their corresponding text descriptions are close together, while mismatched pairs are far apart. In this way, the Conceptual Captions dataset serves as an ideal platform for training and evaluating our model to achieve multimodal learning objectives.\n",
    "\n",
    "2. Common Objects in Context dataset\n",
    "\n",
    "This dataset is a benchmark for object detection and instance segmentation tasks, containing a large number of images with precise annotations for 80 categories, including both bounding boxes and pixel-level masks. We will use the COCO dataset to train object detection and segmentation models to ensure that the models can efficiently recognize and annotate objects in images. Additionally, this dataset will be used to evaluate the model’s performance across various scenarios.\n",
    "\n",
    "#### iii. What are the types of analytical frameworks that you would be using?\n",
    "1. Descriptive Analytics: We will identify and understand the current issues and limitations in existing annotation systems.\n",
    "* Highlight the inefficiencies of manual annotation, such as high cost, time consumption, and human error.\n",
    "\n",
    "* Examine the limitations of existing automated annotation systems (e.g., YOLO, traditional supervised models), including their reliance on extensive labeled data and inability to handle unseen categories or domain shifts.\n",
    "\n",
    "* Analyze the specific gaps in performance for target tasks.\n",
    "\n",
    "2. Predictive Analytics: We will forecast the performance of the proposed annotation system in various scenarios and evaluate its adaptability to unseen data or domains.\n",
    "\n",
    "* OwlViT-based Zero-shot Prediction: Use OwlViT’s pre-trained embeddings to predict the relevance of image-text pairs, assessing its ability to generalize to unseen categories.\n",
    "\n",
    "* SAM-based Segmentation Performance: Predict the accuracy of SAM in segmenting objects under various conditions, such as different object sizes, occlusions, or lighting.\n",
    "\n",
    "* Image-Text Matching: Predict the alignment quality of shared embeddings generated by OwlViT, particularly for noisy data.\n",
    "\n",
    "* Segmentation Accuracy: Predict the performance of SAM on unseen datasets or specific tasks and using metrics like IoU.\n",
    "\n",
    "* Use statistical modeling to evaluate prediction accuracy and machine learning evaluation metrics (F1-score, IoU, recall) to quantify prediction performance.\n",
    "\n",
    "3. Prescriptive Analytics: We will recommend the optimal configurations and workflows for the proposed annotation system to maximize efficiency and accuracy.\n",
    "\n",
    "* Fine-tune OwlViT and SAM for domain-specific tasks (fine-grained segmentation, industrial detection).\n",
    "\n",
    "* Identify optimal hyperparameters for the combined pipeline (embedding dimensions, learning rate).\n",
    "\n",
    "* Try to develop interactive annotation workflows where users can refine predictions with minimal manual input.\n",
    "\n",
    "* Implement cost-benefit analysis to balance annotation quality, computational expense, and human efforts.\n",
    "\n",
    "* Use simulation and experimentation to identify the most effective pipeline configurations for diverse datasets and tasks.\n",
    "\n",
    "* Recommend specific deployment strategies (cloud-based vs local deployment) based on resource availability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839dea7",
   "metadata": {},
   "source": [
    "# Data Preprocess\n",
    "## Overview of COCO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efb2d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=9.82s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "\n",
    "# import coco dataset\n",
    "coco_annotation_path = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "coco = COCO(coco_annotation_path)\n",
    "\n",
    "def convert_bbox_format(bbox):\n",
    "    \"\"\"Convert COCO bbox [x_min, y_min, width, height] format to [x_center, y_center, width, height] format.\"\"\"\n",
    "    x_min, y_min, width, height = bbox\n",
    "    x_center = x_min + width / 2\n",
    "    y_center = y_min + height / 2\n",
    "    return [x_center, y_center, width, height]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bfcf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Categories: 80\n",
      "Names of Categories: ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "categories = coco.loadCats(coco.getCatIds())\n",
    "num_categories = len(categories)\n",
    "category_names = [cat['name'] for cat in categories]\n",
    "print(f\"Number of Categories: {num_categories}\")\n",
    "print(f\"Names of Categories: {category_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d84d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 118287\n"
     ]
    }
   ],
   "source": [
    "num_images = len(coco.getImgIds())\n",
    "print(f\"Number of Images: {num_images}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97f57169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Annotations: 860001\n"
     ]
    }
   ],
   "source": [
    "num_annotations = len(coco.getAnnIds())\n",
    "print(f\"Number of Annotations: {num_annotations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76d5e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Annotations per Images: 7.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "image_ids = coco.getImgIds()\n",
    "num_objects_per_image = [len(coco.getAnnIds(imgIds=[img_id])) for img_id in image_ids]\n",
    "mean_objects_per_image = np.mean(num_objects_per_image)\n",
    "print(f\"Average Number of Annotations per Images: {mean_objects_per_image:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef2c6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hot dog: 2918 objects\n",
      "dog: 5508 objects\n",
      "potted plant: 8652 objects\n",
      "tv: 5805 objects\n",
      "bird: 10806 objects\n",
      "cat: 4768 objects\n",
      "horse: 6587 objects\n",
      "sheep: 9509 objects\n",
      "cow: 8147 objects\n",
      "bottle: 24342 objects\n",
      "couch: 5779 objects\n",
      "chair: 38491 objects\n",
      "dining table: 15714 objects\n",
      "bicycle: 7113 objects\n",
      "car: 43867 objects\n",
      "motorcycle: 8725 objects\n",
      "airplane: 5135 objects\n",
      "bus: 6069 objects\n",
      "train: 4571 objects\n",
      "boat: 10759 objects\n",
      "person: 262465 objects\n",
      "stop sign: 1983 objects\n",
      "umbrella: 11431 objects\n",
      "tie: 6496 objects\n",
      "sports ball: 6347 objects\n",
      "sandwich: 4373 objects\n",
      "bed: 4192 objects\n",
      "cell phone: 6434 objects\n",
      "refrigerator: 2637 objects\n",
      "clock: 6334 objects\n",
      "toothbrush: 1954 objects\n",
      "truck: 9973 objects\n",
      "traffic light: 12884 objects\n",
      "fire hydrant: 1865 objects\n",
      "parking meter: 1285 objects\n",
      "bench: 9838 objects\n",
      "elephant: 5513 objects\n",
      "giraffe: 5131 objects\n",
      "frisbee: 2682 objects\n",
      "skis: 6646 objects\n",
      "snowboard: 2685 objects\n",
      "kite: 9076 objects\n",
      "baseball bat: 3276 objects\n",
      "baseball glove: 3747 objects\n",
      "skateboard: 5543 objects\n",
      "surfboard: 6126 objects\n",
      "tennis racket: 4812 objects\n",
      "wine glass: 7913 objects\n",
      "cup: 20650 objects\n",
      "fork: 5479 objects\n",
      "knife: 7770 objects\n",
      "spoon: 6165 objects\n",
      "bowl: 14358 objects\n",
      "banana: 9458 objects\n",
      "apple: 5851 objects\n",
      "orange: 6399 objects\n",
      "broccoli: 7308 objects\n",
      "carrot: 7852 objects\n",
      "pizza: 5821 objects\n",
      "donut: 7179 objects\n",
      "cake: 6353 objects\n",
      "toilet: 4157 objects\n",
      "laptop: 4970 objects\n",
      "mouse: 2262 objects\n",
      "remote: 5703 objects\n",
      "keyboard: 2855 objects\n",
      "microwave: 1673 objects\n",
      "oven: 3334 objects\n",
      "toaster: 225 objects\n",
      "sink: 5610 objects\n",
      "book: 24715 objects\n",
      "vase: 6613 objects\n",
      "scissors: 1481 objects\n",
      "teddy bear: 4793 objects\n",
      "hair drier: 198 objects\n",
      "backpack: 8720 objects\n",
      "handbag: 12354 objects\n",
      "suitcase: 6192 objects\n",
      "zebra: 5303 objects\n",
      "bear: 1294 objects\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "category_counts = collections.Counter()\n",
    "for ann in coco.loadAnns(coco.getAnnIds()):\n",
    "    category_counts[ann[\"category_id\"]] += 1\n",
    "\n",
    "for cat_id, count in category_counts.items():\n",
    "    cat_name = coco.loadCats([cat_id])[0][\"name\"]\n",
    "    print(f\"{cat_name}: {count} objects\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30867c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate the distribution of object areas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "area_list = [ann['area'] for ann in coco.loadAnns(coco.getAnnIds())]\n",
    "plt.hist(area_list, bins=50, log=True)\n",
    "plt.xlabel('Object Area')\n",
    "plt.ylabel('Number')\n",
    "plt.title('Histogram of Object Area')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc31f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.48s)\n",
      "creating index...\n",
      "index created!\n",
      "Starting data cleaning and augmentation...\n",
      "Number of images without annotations: 1021\n",
      "Number of non-duplicate annotations: 859886\n",
      "Number of annotations after filtering small objects: 784868\n",
      "Data cleaning and augmentation completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.ops import box_iou\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class COCOPipeline:\n",
    "    def __init__(self, annotation_path, iou_threshold=0.9, min_area=100, augment=True):\n",
    "        \"\"\"\n",
    "        COCO Data Cleaning and Augmentation Pipeline.\n",
    "        :param annotation_path: Path to COCO JSON annotation file\n",
    "        :param iou_threshold: IoU threshold to remove duplicate annotations\n",
    "        :param min_area: Minimum area to keep an object\n",
    "        :param augment: Whether to apply data augmentation\n",
    "        \"\"\"\n",
    "        self.coco = COCO(annotation_path)\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.min_area = min_area\n",
    "        self.augment = augment\n",
    "        self.filtered_images = []\n",
    "        self.filtered_annotations = []\n",
    "\n",
    "    def remove_no_annotation_images(self):\n",
    "        \"\"\"Removes images without annotations\"\"\"\n",
    "        annotated_imgs = {ann[\"image_id\"] for ann in self.coco.loadAnns(self.coco.getAnnIds())}\n",
    "        all_imgs = set(self.coco.getImgIds())\n",
    "        empty_imgs = all_imgs - annotated_imgs\n",
    "\n",
    "        print(f\"Number of images without annotations: {len(empty_imgs)}\")\n",
    "        self.filtered_images = [img for img in self.coco.loadImgs(list(annotated_imgs))]\n",
    "\n",
    "    def remove_duplicate_annotations(self):\n",
    "        \"\"\"Removes duplicate annotations (IoU > iou_threshold)\"\"\"\n",
    "        image_to_annotations = {}\n",
    "        for ann in self.coco.loadAnns(self.coco.getAnnIds()):\n",
    "            img_id = ann[\"image_id\"]\n",
    "            if img_id not in image_to_annotations:\n",
    "                image_to_annotations[img_id] = []\n",
    "            image_to_annotations[img_id].append(ann)\n",
    "\n",
    "        filtered_annotations = []\n",
    "        for img_id, anns in image_to_annotations.items():\n",
    "            if len(anns) < 2:\n",
    "                filtered_annotations.extend(anns)\n",
    "                continue  \n",
    "\n",
    "            # Get bounding boxes and categories\n",
    "            bboxes = torch.tensor([self.convert_bbox_format(ann[\"bbox\"]) for ann in anns])\n",
    "            categories = [ann[\"category_id\"] for ann in anns]\n",
    "\n",
    "            # Compute IoU matrix\n",
    "            iou_matrix = box_iou(bboxes, bboxes)\n",
    "\n",
    "            # Store indices of annotations to keep\n",
    "            keep = set(range(len(anns)))\n",
    "            for i in range(len(anns)):\n",
    "                for j in range(i + 1, len(anns)):\n",
    "                    if iou_matrix[i, j] > self.iou_threshold and categories[i] == categories[j]:\n",
    "                        if j in keep:\n",
    "                            keep.remove(j)\n",
    "\n",
    "            filtered_annotations.extend([anns[i] for i in keep])\n",
    "\n",
    "        self.filtered_annotations = filtered_annotations\n",
    "        print(f\"Number of non-duplicate annotations: {len(self.filtered_annotations)}\")\n",
    "\n",
    "    def filter_small_objects(self):\n",
    "        \"\"\"Removes objects with area smaller than min_area\"\"\"\n",
    "        self.filtered_annotations = [ann for ann in self.filtered_annotations if ann['area'] >= self.min_area]\n",
    "        print(f\"Number of annotations after filtering small objects: {len(self.filtered_annotations)}\")\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        \"\"\"Preprocesses an image by resizing, normalizing, and applying data augmentation\"\"\"\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "\n",
    "        if self.augment:\n",
    "            image = self.apply_augmentation(image)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),  # Resize to a fixed size\n",
    "            transforms.ToTensor(),  # Convert to PyTorch Tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "        ])\n",
    "        return transform(Image.fromarray(image))\n",
    "\n",
    "    def apply_augmentation(self, image):\n",
    "        \"\"\"Applies data augmentation using Albumentations\"\"\"\n",
    "        augment = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),  # 50% probability of horizontal flip\n",
    "            A.RandomRotate90(p=0.5),  # 90-degree rotation\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),  # Color jitter\n",
    "            A.Cutout(num_holes=3, max_h_size=20, max_w_size=20, p=0.5),  # Cutout augmentation\n",
    "        ])\n",
    "        return augment(image=image)['image']\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_bbox_format(bbox):\n",
    "        \"\"\"Converts COCO format [x, y, w, h] to [x1, y1, x2, y2]\"\"\"\n",
    "        x, y, w, h = bbox\n",
    "        return [x, y, x + w, y + h]\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Executes the full pipeline\"\"\"\n",
    "        print(\"Starting data cleaning and augmentation...\")\n",
    "        self.remove_no_annotation_images()\n",
    "        self.remove_duplicate_annotations()\n",
    "        self.filter_small_objects()\n",
    "        print(\"Data cleaning and augmentation completed!\")\n",
    "\n",
    "# Run the COCO preprocessing pipeline\n",
    "annotation_path = \"datasets/coco/annotations/instances_train2017.json\"\n",
    "coco_pipeline = COCOPipeline(annotation_path, augment=True)\n",
    "coco_pipeline.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6835343b9a0ec9fe",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "## 1. CLIP (Contrastive Language-Image Pretraining)\n",
    "\n",
    "CLIP, proposed by OpenAI, is a **multi-modal learning model** that associates **images** and **text**, enabling **zero-shot classification and retrieval**.\n",
    "\n",
    "### CLIP's Internal Structure\n",
    "\n",
    "CLIP consists of **two encoders**:\n",
    "\n",
    "- **Vision Encoder**\n",
    "    - Uses **ViT (Vision Transformer)** or ResNet to extract image features\n",
    "    - Converts images into **fixed-dimensional feature vectors** (typically 512-dimensional)\n",
    "\n",
    "- **Text Encoder**\n",
    "    - Uses a **Transformer-based architecture** (similar to GPT-2)\n",
    "    - Converts input text into **feature vectors of the same dimension** (512-dimensional)\n",
    "\n",
    "### CLIP's Computation Process\n",
    "\n",
    "1. **Input Image and Text**\n",
    "    - The image is processed through the **ViT vision encoder**, extracting **image feature vectors**\n",
    "    - The text is processed through the **Transformer text encoder**, extracting **text feature vectors**\n",
    "\n",
    "2. **Similarity Computation**\n",
    "    - Computes the **cosine similarity** between image and text features to match **the most relevant text description**\n",
    "    - Through **contrastive learning**, CLIP maximizes similarity for correct matches and minimizes similarity for incorrect ones\n",
    "\n",
    "### Limitations of CLIP\n",
    "\n",
    "- **Only performs image-text matching**, without directly **outputting bounding boxes**, making it unsuitable for object detection\n",
    "- Requires **additional region proposals**, increasing computational complexity\n",
    "\n",
    "---\n",
    "\n",
    "## 2. OwlViT (Open-World Learning Vision Transformer)\n",
    "\n",
    "OwlViT, developed by Google, is a **ViT-based model for open-vocabulary object detection**, capable of **directly predicting bounding boxes**, overcoming CLIP’s limitations.\n",
    "\n",
    "### OwlViT's Internal Structure\n",
    "\n",
    "OwlViT is an improved version of **Detection Transformer (DETR) + ViT**, consisting of three main components:\n",
    "\n",
    "1. **ViT Vision Encoder**\n",
    "    - Similar to CLIP's ViT encoder, it converts input images into **visual feature vectors (tokens)**\n",
    "    - Unlike CLIP, OwlViT requires **region-based information**, so it includes **positional encoding** to support object detection\n",
    "\n",
    "2. **Cross-Modality Transformer**\n",
    "    - Works similarly to CLIP’s **contrastive learning mechanism**, but not only computes similarity—it also **outputs object detection bounding boxes**\n",
    "    - The input text acts as a **query**, interacting with visual features to **localize target regions**\n",
    "\n",
    "3. **Detection Head**\n",
    "    - Directly predicts the **bounding box coordinates and confidence scores** for each candidate region\n",
    "    - The confidence scores are used for **Non-Maximum Suppression (NMS)** to refine the results\n",
    "\n",
    "### OwlViT's Computation Process\n",
    "\n",
    "1. **Input Image and Text**\n",
    "    - The image is processed through ViT to extract features, while the text is encoded into feature vectors\n",
    "2. **Cross-Modal Interaction**\n",
    "    - Computes the matching score between text and image features to generate candidate object regions\n",
    "3. **Bounding Box Prediction**\n",
    "    - The detection head outputs **bounding box coordinates and confidence scores**\n",
    "4. **Post-Processing**\n",
    "    - **NMS filters out low-confidence boxes** and **merges high IoU boxes** to improve accuracy\n",
    "\n",
    "### Advantages of OwlViT\n",
    "\n",
    "**Directly generates bounding boxes from text queries**, without needing region proposals like CLIP\n",
    "**Supports open-vocabulary object detection**, recognizing **zero-shot categories**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. SAM (Segment Anything Model)\n",
    "\n",
    "SAM, developed by Meta, is a **high-precision object segmentation model** that generates object masks based on **prompts**.\n",
    "\n",
    "### SAM's Internal Structure\n",
    "\n",
    "SAM consists of **three main components**:\n",
    "\n",
    "1. **ViT Vision Encoder**\n",
    "    - Like CLIP and OwlViT, SAM also uses **ViT** for image feature extraction\n",
    "    - However, SAM requires **high-resolution feature maps**, so it adopts a **high-capacity ViT variant (ViT-Huge)**\n",
    "\n",
    "2. **Prompt Encoder**\n",
    "    - SAM supports **various types of prompts**, including:\n",
    "        - **Point**: The user clicks on an object, and SAM predicts the mask for that region\n",
    "        - **Box**: The user provides a bounding box, and SAM generates a precise mask\n",
    "        - **Text**: Future versions may support text-based prompts\n",
    "    - In this project, we use **OwlViT-generated bounding boxes as SAM’s input**\n",
    "\n",
    "3. **Mask Decoder**\n",
    "    - Combines **ViT visual features + prompt information** to generate the **final segmentation mask**\n",
    "\n",
    "### SAM's Computation Process\n",
    "\n",
    "1. **Input Image and Bounding Box**\n",
    "    - OwlViT first generates bounding boxes, which are then used as **prompts** for SAM\n",
    "2. **Feature Extraction**\n",
    "    - SAM extracts **high-resolution image features** using ViT\n",
    "3. **Prompt Encoding**\n",
    "    - SAM processes the bounding box information and adjusts the prediction scope\n",
    "4. **Mask Generation**\n",
    "    - The Mask Decoder outputs **high-quality object segmentation masks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374ef31-696a-4ecf-8de8-23d80afffa51",
   "metadata": {},
   "source": [
    "# Model Selection – CLIP + SAM vs. OwlViT + SAM\n",
    "\n",
    "## Initial Attempt: CLIP + SAM\n",
    "\n",
    "### Why Initially Choose CLIP?\n",
    "\n",
    "- CLIP is a powerful **vision-language model** that understands the relationship between images and text, supporting **zero-shot learning**.\n",
    "- It is well-suited for **image classification** and **open-category object recognition**, allowing it to handle **unseen categories**.\n",
    "\n",
    "### Issues with CLIP + SAM\n",
    "\n",
    "- **CLIP cannot directly provide bounding boxes**, only assessing the similarity between an entire image and a text query.\n",
    "- **Requires an additional region proposal algorithm**:\n",
    "    - Since CLIP cannot directly detect objects, **Selective Search, edge detection**, or other methods must first generate **candidate regions**.\n",
    "    - Each candidate region is then matched with CLIP using **text similarity**, selecting the highest-matching region as the detection result.\n",
    "- **Multi-step process increases computational complexity**:\n",
    "    1. Generate multiple candidate regions.\n",
    "    2. Use CLIP to compute the similarity score for each region with the text query.\n",
    "    3. Select the highest-matching region as the detection result.\n",
    "    4. Use SAM for object segmentation.\n",
    "- **Accuracy limitations**:\n",
    "    - The quality of candidate region proposals determines final detection performance, making it prone to missing objects or false detections.\n",
    "    - Additional computation steps lead to slower inference speed.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Choice: OwlViT + MobileSAM\n",
    "\n",
    "### Why Switch to OwlViT?\n",
    "\n",
    "- **OwlViT is an open-vocabulary object detection model** that can **directly generate bounding boxes from text queries**, eliminating the need for additional region proposals.\n",
    "- **End-to-end object detection**: Given an image and a text query, the model directly outputs **bounding boxes**, avoiding CLIPs **multi-step processing**, improving detection speed and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow of OwlViT + MobileSAM\n",
    "\n",
    "1. **Input image + text query**.\n",
    "2. **OwlViT processes the image and text**, directly outputting **bounding boxes + confidence scores**.\n",
    "3. **Filter out low-confidence bounding boxes**, then apply **Non-Maximum Suppression (NMS)** to remove overlapping boxes.\n",
    "4. **Merge high IoU (Intersection over Union) detection boxes**, improving stability.\n",
    "5. **MobileSAM receives the final bounding boxes** and generates high-quality segmentation masks.\n",
    "6. **Final output**: Image with accurately segmented objects.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Is OwlViT + SAM Superior?\n",
    "\n",
    "| **Comparison** | **CLIP + SAM** | **OwlViT + SAM** |\n",
    "| --- | --- | --- |\n",
    "| **Zero-shot detection ability** | (Image-level only) |  (Object-level, direct detection) |\n",
    "| **Can output bounding boxes?** |  Requires extra steps |  Direct output |\n",
    "| **Requires region proposals?** |  Yes |  No |\n",
    "| **Merging high IoU boxes** |  No |  Using NMS and box merging |\n",
    "| **Detection accuracy** |  Depends on region proposal quality |  Optimized for precision |\n",
    "| **Computation speed** |  High due to extra steps |  More efficient |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34695b3-8bf2-43ae-88a7-7baf9b04f13a",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "\n",
    "Our goal is to evaluate the **object detection** and **segmentation performance** of **OwlViT + MobileSAM**. The evaluation focuses on three key aspects: **detection accuracy, segmentation quality, and computational efficiency**.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Object Detection Evaluation Metrics\n",
    "\n",
    "The detection capability of **OwlViT** determines the final segmentation quality. Therefore, we evaluate the following metrics:\n",
    "\n",
    "### **1.1 IoU (Intersection over Union)**\n",
    "- Measures the **overlap ratio** between the predicted bounding box and the ground truth bounding box:\n",
    "$$\n",
    "\\text{IoU} = \\frac{\\text{Area of intersection}}{\\text{Area of union}}\n",
    "$$\n",
    "- **IoU > 0.5** is considered a correct detection (standard threshold).\n",
    "- **IoU > 0.9** represents high-quality detection.\n",
    "\n",
    "### **1.2 mAP (Mean Average Precision)(Later)**\n",
    "- Evaluates object detection performance across **different IoU thresholds**, calculating the average **AP** over multiple IoU values.\n",
    "- AP is calculated as follows:\n",
    "    - Compute the **area under the Precision-Recall curve**:\n",
    "    $$\n",
    "    \\text{AP} = \\int_{0}^{1} P(R) \\, dR\n",
    "    $$\n",
    "    - The final **mAP (Mean AP)** is obtained by averaging AP across all categories.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Segmentation Evaluation Metrics\n",
    "\n",
    "**MobileSAM** is responsible for object segmentation. The evaluation metrics include:\n",
    "\n",
    "### **2.1 mIoU (Mean Intersection over Union)**\n",
    "- Computes the average IoU between the predicted mask and the ground truth mask:\n",
    " $$\n",
    "\\text{mIoU} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\text{Area of intersection}}{\\text{Area of union}}\n",
    "$$\n",
    "- **mIoU > 0.8** represents high-quality segmentation.\n",
    "\n",
    "### **2.2 Dice Coefficient**\n",
    "- Measures the similarity between two regions:\n",
    "$$\n",
    "\\text{Dice} = \\frac{2 \\times |A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "- A Dice score closer to 1 indicates better segmentation performance.\n",
    "---\n",
    "\n",
    "## 3. Computational Efficiency\n",
    "\n",
    "### **3.1 FPS (Frames Per Second)**\n",
    "- Measures **the number of images processed per second**, assessing the system’s real-time performance:\n",
    "$$\n",
    "\\text{FPS} = \\frac{\\text{Number of processed images}}{\\text{Total time (seconds)}}\n",
    "$$\n",
    "- **Target values**:\n",
    "    - FPS > 2 for batch processing\n",
    "    - FPS > 10 for real-time applications\n",
    "\n",
    "### **3.2 Inference Time**\n",
    "- Measures the total inference time for **OwlViT + MobileSAM** on a **single image**:\n",
    "$$\n",
    "T_{\\text{total}} = T_{\\text{OwlViT}} + T_{\\text{MobileSAM}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94e528-3f70-49f4-ac2a-082f878f2b82",
   "metadata": {},
   "source": [
    "# **Next Steps**\n",
    "\n",
    "After the mid-term report, our primary goal is to **optimize the post-processing pipeline for detection and segmentation to improve detection accuracy, segmentation quality, and inference efficiency**. The next optimization steps mainly include **improving confidence filtering, NMS, and high IoU box merging strategies**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Object Detection Optimization**\n",
    "\n",
    "### **1.1 Confidence Filtering Improvement**\n",
    "\n",
    "Currently, `filter_boxes_by_score` uses `sigmoid(logits)` to compute confidence and dynamically sets the threshold. However, there are some issues:\n",
    "\n",
    "- **Low-confidence cases may result in filtering out all targets**\n",
    "- **A fixed `scores.mean() - scores.std()` threshold may lead to false detections**\n",
    "\n",
    "#### **Optimization Plan**\n",
    "\n",
    "- Apply **Z-Score normalization** to logits to make their distribution more stable:\n",
    "  \n",
    "  $$\n",
    "  \\text{scores} = \\frac{\\text{logits} - \\text{logits.mean()}}{\\text{logits.std()} + 1e-6}\n",
    "  $$\n",
    "\n",
    "  Then, compute `sigmoid(scores)` to ensure reasonable confidence values.\n",
    "\n",
    "- **Adjust the dynamic threshold calculation**:\n",
    "\n",
    "  $$\n",
    "\\text{threshold} = \\max(\\text{min-threshold}, \\text{scores.mean} - 0.5 \\cdot \\text{scores.std})\n",
    "$$\n",
    "\n",
    "\n",
    "  **Objective**: Improve the precision of bounding box filtering, reducing false positives and false negatives.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 NMS Strategy Optimization**\n",
    "\n",
    "Currently, `apply_nms` uses **Hard-NMS** (based on `torchvision.ops.nms()`), which directly removes high-IoU boxes. This can cause:\n",
    "\n",
    "- **False negatives**, where adjacent objects are incorrectly removed\n",
    "- **A drastic reduction in the number of detected boxes**, negatively affecting recall\n",
    "\n",
    "#### **Optimization Plan**\n",
    "\n",
    "- **Introduce Soft-NMS**\n",
    "    - Instead of directly removing boxes, apply **exponential decay** to confidence scores based on IoU:\n",
    "  \n",
    "      $$\n",
    "      \\text{scores} = \\text{scores} \\times e^{- (\\text{IoU}^2) / \\sigma}\n",
    "      $$\n",
    "\n",
    "    - **Low IoU boxes retain more confidence**, while **high IoU boxes decay more significantly**, preserving more detection information.\n",
    "\n",
    "- **Applicable scenarios**:\n",
    "    - **Crowded object detection** (e.g., pedestrian or vehicle detection)\n",
    "    - **Reduce false negatives and improve recall**\n",
    "\n",
    "---\n",
    "\n",
    "### **1.3 High-IoU Box Merging Optimization**\n",
    "\n",
    "Currently, `merge_high_iou_boxes` applies a confidence-weighted averaging method. However, there are issues:\n",
    "\n",
    "- **Merging multiple high-IoU boxes may shift the final box away from the original target**\n",
    "- **Direct weighted averaging may distort box dimensions**\n",
    "$$ $$\n",
    "---\n",
    "\n",
    "## **2. Segmentation Optimization**\n",
    "\n",
    "### **2.1 Multi-Mask Optimization in MobileSAM**\n",
    "\n",
    "#### **Current Issues**\n",
    "- Using `multimask_output=False` may result in missing small objects.\n",
    "\n",
    "#### **Optimization Plan**\n",
    "- Enable `multimask_output=True` to generate multiple masks and select the best one.\n",
    "- **Objective**: Ensure complete segmentation of targets in complex scenes.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Mask Post-Processing**\n",
    "\n",
    "- **Add Mask Area Filtering**\n",
    "    - **Remove very small masks** (e.g., those with <500 pixels) to avoid detecting noise.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Evaluation Expansion**\n",
    "\n",
    "### **3.1 Testing on Different Datasets**\n",
    "\n",
    "- **Zero-Shot Evaluation**:\n",
    "    - The model has primarily been evaluated on COCO; testing on **unseen categories** is necessary.\n",
    "- **Cross-Dataset Testing**:\n",
    "    - Plan to evaluate on **LVIS and Object365** to ensure generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 More Detailed Performance Analysis**\n",
    "\n",
    "- **IoU Histogram Analysis**\n",
    "    - Analyze the distribution of predicted vs. ground truth IoU to refine detection accuracy.\n",
    "- **Segmentation Quality vs. Object Size Analysis**\n",
    "    - Observe `mIoU` performance across different object scales to enhance segmentation stability.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Documentation & Presentation**\n",
    "\n",
    "### **4.1 Visualization of Experimental Results**\n",
    "\n",
    "- **Plot Performance Comparison Charts**\n",
    "    - **mAP (AP@50, AP@75, AP@[0.5:0.95])** to measure improvements before and after optimization.\n",
    "    - **mIoU vs. Object Size Comparison** to analyze segmentation performance gains.\n",
    "    - **Inference Time vs. Batch Size** to optimize throughput.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.2 Report Writing**\n",
    "\n",
    "- **Organize experimental data** and write a **detailed comparative analysis** to ensure optimizations are data-driven.\n",
    "- **Summarize final model improvements, refinements, and future directions**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888d2fe",
   "metadata": {},
   "source": [
    "\n",
    "## Bibliography\n",
    "\n",
    "[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” arXiv preprint arXiv:2103.00020, 2021. [Online]. Available: https://arxiv.org/abs/2103.00020. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[2] R. Sapkota, A. Paudel, and M. Karkee, “Zero-shot automatic annotation and instance segmentation using LLM-generated datasets: Eliminating field imaging and manual annotation for deep learning model development,” arXiv preprint arXiv:2411.11285, 2024. [Online]. Available: https://arxiv.org/abs/2411.11285. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[3] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional networks for biomedical image segmentation,” arXiv preprint arXiv:1505.04597, 2015. [Online]. Available: https://arxiv.org/abs/1505.04597. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[4] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” OpenAI, 2019. [Online]. Available: [https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf]. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[5] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once: Unified, real-time object detection,” arXiv preprint arXiv:1506.02640, 2015. [Online]. Available: https://arxiv.org/abs/1506.02640. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[6] S. Atito, M. Awais, and J. Kittler, “SiT: Self-supervised vision transformer,” arXiv preprint arXiv:2104.03602, 2021. [Online]. Available: https://arxiv.org/abs/2104.03602. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[7] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, P. Dollár, and R. Girshick, “Segment Anything,” arXiv preprint arXiv:2304.02643, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2304.02643. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[8] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft COCO: Common objects in context,” arXiv preprint arXiv:1405.0312, 2014. [Online]. Available: https://arxiv.org/abs/1405.0312. [Accessed: Jan. 29, 2025].\n",
    "\n",
    "[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint arXiv:1706.03762, 2017. [Online]. Available: https://doi.org/10.48550/arXiv.1706.03762. [Accessed: Jan. 29, 2025].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271192da6f16d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
